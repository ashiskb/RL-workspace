{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Reinforcement Learning with OpenAI Gym\n",
    "## About Gym\n",
    "`Gym` is an open source Python library for developing and comparing `reinforcement learning` algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. \n",
    "* `Gym` documentation website is located [here](https://www.gymlibrary.dev/). \n",
    "* `Gym` also has a discord server for development purposes that you can join [here](https://discord.gg/nHg2JRN489).\n",
    "* `Gym`'s official developer site is [here](https://github.com/openai/gym).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gym[pong]\n"
     ]
    }
   ],
   "source": [
    "#!setopt no_nomatch\n",
    "#!pip install gym[pong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with `CartPole-v0` environment\n",
    "* This environment is from the [classic control group](https://www.gymlibrary.dev/environments/classic_control/)\n",
    "* Please note the following is a reference of `CartPole-v1` instead of `CartPole-v0`. They both share a lot of similarity, and few subtle differences. For example, in `CartPole-v1` reward threshold is set to 475 whereas in `Cartpole-v0` it was set to 195.\n",
    "\n",
    "![Cartpole-v0](figs/Cartpole-v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with `CartPole-v0` environment\n",
    "* This environment is from the [classic control group](https://www.gymlibrary.dev/environments/classic_control/)\n",
    "* **Goal** is to control the cart (i.e., platform) with a pole attached by its bottom prt.\n",
    "* **Trick**: The pole tends to fall right or left and you would need to balance it by moving the cart to the right or left on every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashis/venv-directory/venv-ml-p3.10/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## State space (observable)\n",
    "* The observation of the environment is 4 floating point numbers: [position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "    1. x-coordinate of the pole's center of mass\n",
    "    2. the pole's speed\n",
    "    3.  the pole's angle to the cart/platform. the pole angle in radians (1 radian = 57.295 degrees)\n",
    "    4. the pole's rotation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs = [ 0.04054569 -0.03567298  0.0173007   0.02051942]\n"
     ]
    }
   ],
   "source": [
    "obs,info = env.reset()\n",
    "print('obs = {}'.format(obs))\n",
    "#Example printout:\n",
    "# obs = [-0.02007766 -0.00363281 -0.0034504  -0.02222458]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem is to find the `best action` per step\n",
    "* We need to convert these 4 observations to into actions. \n",
    "* But, how do we learn to balance this system without knowing the exact meaning of the observed 4 numbers by getting the reward? \n",
    "* Here, the reward is 1; and it is given on every time step.\n",
    "* The episode continues until the pole falls.\n",
    "* To get a more accumulated reward, we need to balance the platform, as long as possible, in a way to avoid the pole falling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.action_space = Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print('env.action_space = {}'.format(env.action_space))\n",
    "#Example printout:\n",
    "# env.action_space = Discrete(2)\n",
    "#only 2 actions: 0 or 1, where 0 means pushing the platform to the left, 1 means to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "env.observation_space.high = [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "env.observation_space.low = [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print('env.observation_space = {}'.format(env.observation_space))\n",
    "#Example printout:\n",
    "# env.observation_space = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
    "#The observation space is a 4-D space, and each dimension is as follows:\n",
    "#Num Observation             Min         Max\n",
    "#0   Cart Position           -2.4        2.4\n",
    "#1   Cart Velocity           -Inf        Inf\n",
    "#2   Pole Angle              ~ -41.8°    ~ 41.8°\n",
    "#3   Pole Velocity At Tip    -Inf        Inf\n",
    "#env.observation_space.low and env.observation_space.high which will print the minimum and maximum values for each observation variable.\n",
    "print('env.observation_space.high = {}'.format(env.observation_space.high))\n",
    "print('env.observation_space.low = {}'.format(env.observation_space.low))\n",
    "#Example printout:\n",
    "#env.observation_space.high = [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
    "#env.observation_space.low = [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apply a specific action at a step\n",
    "* How about going left, i.e., action=0 from the action_space?\n",
    "    - result is a `new state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation = [ 0.03983223 -0.23103872  0.01771109  0.3186103 ]\n",
      "reward = 1.0\n",
      "terminated = False\n",
      "truncated = False\n",
      "info = {}\n"
     ]
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(0)\n",
    "print('observation = {}'.format(observation))\n",
    "print('reward = {}'.format(reward))\n",
    "print('terminated = {}'.format(terminated))\n",
    "print('truncated = {}'.format(truncated))\n",
    "print('info = {}'.format(info))\n",
    "#Example printout:\n",
    "#observation = [-0.02728556 -0.22667485 -0.01062018  0.3176722 ]\n",
    "#reward = 1.0\n",
    "#terminated = False\n",
    "#truncated = False\n",
    "#info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apply a random action at a step\n",
    "* The `sample()` returns a random sample from the given/supplied space.\n",
    "* Here below, you can see that we sample from the `action_space`.\n",
    "* The `sample()` can also be used to sample from the `observation_space` as well -- although why would we want to use that here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action = 1\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print('action = {}'.format(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action = 0\n"
     ]
    }
   ],
   "source": [
    "#Let's apply another random action with sampling\n",
    "action = env.action_space.sample()\n",
    "print('action = {}'.format(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action = 1\n"
     ]
    }
   ],
   "source": [
    "#Let's apply another random action with sampling\n",
    "action = env.action_space.sample()\n",
    "print('action = {}'.format(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here is a loop with A Random CartPole-v0 agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#!/Users/ashis/venv-directory/venv-ml-p3.10/bin/python3.10\n",
    "#Please make this python file executable and then run it without passing it to python interpreter\n",
    "#as the the interpreter listed on the first line will be invoked. Good luck!\n",
    "#$ chmod +x CartPole-v0-code3.py\n",
    "#$ ./CartPole-v0-code3.py\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#The CartPole-v0 environment with a random agent\n",
    "# Goal is to control the cart (i.e., platform) with a pole attached by its bottom prt.\n",
    "# Trick: The pole tends to fall right or left and you would need to balance it by moving the cart to the right or left on every step.\n",
    "\n",
    "env = gym.make(\"CartPole-v0\",render_mode='human')\n",
    "\n",
    "#Here below, we created the environment and initialized few variables.\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "print('Episode terminated in {} steps\\nTotal rewards accumulated = {}'.format(total_steps,total_reward))\n",
    "\n",
    "#On average, this random agent takes 12 to 15 steps before the pole falls and the episode ends\n",
    "#Most of the environments in Gym have a `reward boundary`, which is the average reward that the agent should gain during 100 consecutive eposides to solve the environment.\n",
    "#For cartpole, the boundary is 195. That means, on average, the agent must hold the stick for 195 time steps or longer.\n",
    "#So, our random agent's performance is extremely poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for your attention"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": 855,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "width": 1140
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
