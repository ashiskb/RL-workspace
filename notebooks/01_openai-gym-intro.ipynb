{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Reinforcement Learning with OpenAI Gym\n",
    "## About Gym\n",
    "`Gym` is an open source Python library for developing and comparing `reinforcement learning` algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. \n",
    "* `Gym` documentation website is located [here](https://www.gymlibrary.dev/). \n",
    "* `Gym` also has a discord server for development purposes that you can join [here](https://discord.gg/nHg2JRN489).\n",
    "* `Gym`'s official developer site is [here](https://github.com/openai/gym).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installation\n",
    "* `pip install gym`\n",
    "\n",
    "This does not include dependencies for all families of environments (there's a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like `pip install gym[atari]` or use the following to install all dependencies.: \n",
    "* `pip install gym[all]`\n",
    "\n",
    "If the above pip install throws a bash/zsh error, it might be the subscripts not allowed there. You need to set option for that.\n",
    "* `setopt no_nomatch`\n",
    "\n",
    "Then run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gym[pong]\n"
     ]
    }
   ],
   "source": [
    "#!setopt no_nomatch\n",
    "#!pip install gym[pong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Listing available `gym` environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/ashis/venv-directory/venv-ml-p3.10/bin/python3.10\n",
    "#Please make this python file executable and then run it without passing \n",
    "# it to python interpreter as the the interpreter listed on the first line \n",
    "# will be invoked. Good luck!\n",
    "#$ chmod +x list_all_envs_registry.py\n",
    "#$ ./list_all_envs_registry.py\n",
    "\n",
    "from gym import envs\n",
    "#all_envs = envs.registry.all()\n",
    "#env_ids = [env_spec.id for env_spec in all_envs]\n",
    "#pprint(sorted(env_ids))\n",
    "for key in envs.registry.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interacting with the Environment\n",
    "Gym implements the classic “agent-environment loop”:\n",
    "\n",
    "<img src=\"https://www.gymlibrary.dev/_images/AE_loop_dark.png\" style=\"background-color:black;\" width=300> \n",
    "\n",
    "Image courtesy: www.gymlibrary.dev\n",
    "\n",
    "The agent performs some `actions` in the environment (usually by passing some control inputs to the environment, e.g. torque inputs of motors) and observes how the `environment’s state` changes. One such action-observation exchange is referred to as a `timestep`.\n",
    "\n",
    "The goal in Reinforcement Learning (RL) is to manipulate the `environment` in some specific way. \n",
    "\n",
    "For instance, we want the agent to navigate a robot to a specific point in space. \n",
    "* If it succeeds in doing this (or makes some progress towards that goal), it will receive a `positive reward` alongside the observation for this `timestep`. \n",
    "* The reward may also be negative or 0, if the agent did not yet succeed (or did not make any progress). \n",
    "* The agent will then be trained to maximize the reward it accumulates over many timesteps.\n",
    "* After some timesteps, the environment may enter a terminal state. \n",
    "    * For instance, the robot may have crashed! In that case, we want to `reset the environment` to a new initial state. The environment issues a done signal to the agent if it enters such a terminal state. \n",
    "    * Not all done signals must be triggered by a “catastrophic failure”: Sometimes we also want to issue a done signal after a fixed number of timesteps, or if the agent has succeeded in completing some task in the environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agent-Environment loop in `Gym`\n",
    "* Here below are few examples of agent-environment loop in `gym`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LunarLander-v2\n",
    "* Reference [https://www.gymlibrary.dev/environments/box2d/lunar_lander/](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "![lunarlander-v2](figs/lunar_landerv2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LunarLander-v2\n",
    "\n",
    "* This example will run an instance of `LunarLander-v2` environment for `n` timesteps. \n",
    "* Since we pass `render_mode=\"human\"`, you should see a window pop up rendering the environment.\n",
    "* Save the following in a file named `lunarlanderv2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/ashis/venv-directory/venv-ml-p3.10/bin/python3.10\n",
    "#Please make this python file executable and then run it without passing it to python interpreter\n",
    "#as the the interpreter listed on the first line will be invoked. Good luck!\n",
    "#$ chmod +x lunarlanderv2.py\n",
    "#$ ./lunralanderv2.py\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "#number of timestepts\n",
    "n = 500\n",
    "\n",
    "#Since we pass render_mode=\"human\", you should see a window pop up rendering the environment.\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in tqdm(range(n)):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        #break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Action space and Observation (i.e., state) space\n",
    "* Every environment specifies the format of valid actions by providing an `env.action_space` attribute. \n",
    "* Similarly, the format of valid observations is specified by `env.observation_space`. \n",
    "* In the example above we sampled random actions via `env.action_space.sample()`. \n",
    "* Note that we need to seed the action space separately from the environment to ensure reproducible samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for your attention"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv-ml-p3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": 855,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "width": 1140
  },
  "vscode": {
   "interpreter": {
    "hash": "d93dfb006d9629cf586318b929ece614012e52132ab19faf5910bcf241a5c1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
