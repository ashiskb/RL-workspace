{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-gym environment\n",
    "## Action-Environment Loop (From scratch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Non-Gym game development (Part 1)\n",
    "## Goal vs Hole - v0\n",
    "* Simple 2D text-based game with the following environment:\n",
    "\n",
    "```text\n",
    "---------------------------------\n",
    "|         |          |          |\n",
    "|  Start  |          |  Goal    |\n",
    "|         |          |          |\n",
    "---------------------------------\n",
    "|         |          |          |\n",
    "|         |          |  Hole    |\n",
    "|         |          |          |\n",
    "---------------------------------\n",
    "```\n",
    "* The environment is a 2x3 grid.\n",
    "* 2 terminal states: `Goal` and `Hole`; if a player moves into any of the two terminal states, the game is over.\n",
    "* 4 non-terminal states.\n",
    "* **Rewards**\n",
    "    * 0 if on a non-terminal state,\n",
    "    * -100 if on `Hole` state,\n",
    "    * +100 if on `Goal` state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#the imports\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from termcolor import colored\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Let's define a class\n",
    "class Goal_vs_Hole_v0():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def init_reward_table(self):\n",
    "    \"\"\"\n",
    "    0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    ----------------\n",
    "    | 0 | 0 | 100  |\n",
    "    ----------------\n",
    "    | 0 | 0 | -100 |\n",
    "    ----------------\n",
    "    \"\"\"\n",
    "    self.reward_table = np.zeros([self.row, self.col])\n",
    "    self.reward_table[1, 2] = 100.\n",
    "    self.reward_table[4, 2] = -100.\n",
    "\n",
    "Goal_vs_Hole_v0.init_reward_table = init_reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def init_transition_table(self):\n",
    "    \"\"\" TT[state_{i},action] = state_{i+1}\n",
    "    0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    -------------\n",
    "    | 0 | 1 | 2 |\n",
    "    -------------\n",
    "    | 3 | 4 | 5 |\n",
    "    -------------\n",
    "    \"\"\"\n",
    "    self.action_space = [0,1,2,3]\n",
    "    self.observation_space = [0,1,2,3,4,5]\n",
    "    self.observation_space_terminal = [2,5]\n",
    "    self.observation_space_non_terminal = [0,1,3,4]\n",
    "    \n",
    "    self.transition_table = np.zeros([self.row, self.col], dtype=int)\n",
    "\n",
    "    self.transition_table[0, 0] = 0\n",
    "    self.transition_table[0, 1] = 3\n",
    "    self.transition_table[0, 2] = 1\n",
    "    self.transition_table[0, 3] = 0\n",
    "\n",
    "    self.transition_table[1, 0] = 0\n",
    "    self.transition_table[1, 1] = 4\n",
    "    self.transition_table[1, 2] = 2\n",
    "    self.transition_table[1, 3] = 1\n",
    "\n",
    "    # terminal Goal state\n",
    "    self.transition_table[2, 0] = 2\n",
    "    self.transition_table[2, 1] = 2\n",
    "    self.transition_table[2, 2] = 2\n",
    "    self.transition_table[2, 3] = 2\n",
    "\n",
    "    self.transition_table[3, 0] = 3\n",
    "    self.transition_table[3, 1] = 3\n",
    "    self.transition_table[3, 2] = 4\n",
    "    self.transition_table[3, 3] = 0\n",
    "\n",
    "    self.transition_table[4, 0] = 3\n",
    "    self.transition_table[4, 1] = 4\n",
    "    self.transition_table[4, 2] = 5\n",
    "    self.transition_table[4, 3] = 1\n",
    "\n",
    "    # terminal Hole state\n",
    "    self.transition_table[5, 0] = 5\n",
    "    self.transition_table[5, 1] = 5\n",
    "    self.transition_table[5, 2] = 5\n",
    "    self.transition_table[5, 3] = 5\n",
    "    \n",
    "Goal_vs_Hole_v0.init_transition_table = init_transition_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# start of episode\n",
    "def reset(self, start_state=0):\n",
    "    self.state = start_state\n",
    "    return self.state\n",
    "\n",
    "Goal_vs_Hole_v0.reset = reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(self, start_state=0):\n",
    "    # 4 actions\n",
    "    # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    self.col = 4\n",
    "\n",
    "    # 6 states\n",
    "    self.row = 6\n",
    "\n",
    "    # setup the environment\n",
    "    self.q_table = np.zeros([self.row, self.col])\n",
    "    self.init_transition_table()\n",
    "    self.init_reward_table()\n",
    "\n",
    "    # discount factor\n",
    "    self.gamma = 0.9\n",
    "\n",
    "    # 90% exploration, 10% exploitation\n",
    "    self.epsilon = 0.9\n",
    "    \n",
    "    # exploration decays by this factor every episode\n",
    "    self.epsilon_decay = 0.99\n",
    "    # in the long run, 10% exploration, 90% exploitation\n",
    "    # meaning, the agent is never going to bore you with predictable moves :D\n",
    "    self.epsilon_min = 0.1\n",
    "\n",
    "    # reset the environment\n",
    "    self.reset(start_state)\n",
    "    self.is_explore = True\n",
    "    \n",
    "    \n",
    "Goal_vs_Hole_v0.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# agent wins when the goal is reached\n",
    "def is_in_win_state(self):\n",
    "    return self.state == 2\n",
    "Goal_vs_Hole_v0.is_in_win_state = is_in_win_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# execute the action on the environment\n",
    "def step(self, action):\n",
    "    # determine the next_state given state and action\n",
    "    next_state = self.transition_table[self.state, action]\n",
    "\n",
    "    # done is True if next_state is Goal or Hole\n",
    "    done = next_state == 2 or next_state == 5\n",
    "\n",
    "    # reward given the state and action\n",
    "    reward = self.reward_table[self.state, action]\n",
    "\n",
    "    # the enviroment is now in new state\n",
    "    self.state = next_state\n",
    "\n",
    "    return next_state, reward, done\n",
    "\n",
    "Goal_vs_Hole_v0.step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# determine the next action\n",
    "def act(self):\n",
    "    # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "\n",
    "    # action is from exploration, by following the distribution \"epsilon\"\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "        # explore - do random action\n",
    "        self.is_explore = True\n",
    "        \n",
    "        #find valid transitions from current state\n",
    "        valid_actions_from_state = np.where(self.transition_table[self.state,:]!=self.state)\n",
    "        #return np.random.choice(4, 1)[0]  #4C1\n",
    "        return np.random.choice(valid_actions_from_state[0])\n",
    "        \n",
    "    # otherwise,  action is from exploitation\n",
    "    # exploit - choose action with max Q-value\n",
    "    self.is_explore = False\n",
    "\n",
    "    return np.argmax(self.q_table[self.state])\n",
    "\n",
    "Goal_vs_Hole_v0.act = act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Learning - update the Q Table using Q(s, a)\n",
    "def update_q_table(self, state, action, reward, next_state):\n",
    "    # Remember the Bellman equation to update the Q table?\n",
    "    # Q(s, a) = reward + gamma * max_a' Q(s', a')\n",
    "    q_value = reward + self.gamma * np.amax(self.q_table[next_state])\n",
    "\n",
    "    self.q_table[state, action] = q_value\n",
    "    \n",
    "Goal_vs_Hole_v0.update_q_table = update_q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# update Exploration-Exploitation mix\n",
    "def update_epsilon(self, do_test=False):\n",
    "    if do_test:\n",
    "        self.epsilon = 0.0 #absolutely no exploration during test time\n",
    "    else:\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "Goal_vs_Hole_v0.update_epsilon = update_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display agent moving on the grid\n",
    "# Playing with the terminal, so you could see \"kinda\" animation!\n",
    "def print_cell(self, row=0):\n",
    "    print(\"\")\n",
    "    for i in range(13):\n",
    "        j = i - 2\n",
    "        if j in [0, 4, 8]:\n",
    "            if j == 8:\n",
    "                if self.state == 2 and row == 0:\n",
    "                    marker = \"\\033[4mG\\033[0m\"\n",
    "                elif self.state == 5 and row == 1:\n",
    "                    marker = \"\\033[4mH\\033[0m\"\n",
    "                else:\n",
    "                    marker = 'G' if row == 0 else 'H'\n",
    "                color = self.state == 2 and row == 0\n",
    "                color = color or (self.state == 5 and row == 1)\n",
    "                color = 'red' if color else 'blue'\n",
    "                print(colored(marker, color), end='')\n",
    "            elif self.state in [0, 1, 3, 4]:\n",
    "                cell = [(0, 0, 0), (1, 0, 4), (3, 1, 0), (4, 1, 4)]\n",
    "                marker = '_' if (self.state, row, j) in cell else ' '\n",
    "                print(colored(marker, 'red'), end='')\n",
    "            else:\n",
    "                print(' ', end='')\n",
    "        elif i % 4 == 0:\n",
    "            print('|', end='')\n",
    "        else:\n",
    "            print(' ', end='')\n",
    "    print(\"\")\n",
    "    \n",
    "Goal_vs_Hole_v0.print_cell = print_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display mode and action of agent\n",
    "def print_world(self, action, step):\n",
    "    actions = {0: \"(Left)\", 1: \"(Down)\", 2: \"(Right)\", 3: \"(Up)\"}\n",
    "    explore = \"Explore\" if self.is_explore else \"Exploit\"\n",
    "    print(\"Step\", step, \":\", explore, actions[action])\n",
    "    for _ in range(13):\n",
    "        print('-', end='')\n",
    "    self.print_cell()\n",
    "    for _ in range(13):\n",
    "        print('-', end='')\n",
    "    self.print_cell(row=1)\n",
    "    for _ in range(13):\n",
    "        print('-', end='')\n",
    "    print(\"\")\n",
    "\n",
    "Goal_vs_Hole_v0.print_world = print_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print Q Table contents\n",
    "def print_q_table(self):\n",
    "    print(\"Q-Table (Epsilon: %0.2f)\" % self.epsilon)\n",
    "    print(self.q_table)\n",
    "Goal_vs_Hole_v0.print_q_table = print_q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# save member variables in a pickle\n",
    "def save_world(self, joblib_file='joblibs/goal_vs_hole_world_v0.joblib'):\n",
    "    dump([self.col, self.row, self.q_table, self.gamma, self.epsilon, \n",
    "        self.epsilon_decay, self.epsilon_min, self.is_explore], joblib_file)\n",
    "\n",
    "Goal_vs_Hole_v0.save_world = save_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load member variables from a pickle file\n",
    "def load_world(self, joblib_file = 'joblibs/goal_vs_hole_world_v0.joblib'):\n",
    "    [self.col, self.row, self.q_table, self.gamma, self.epsilon,\n",
    "            self.epsilon_decay, self.epsilon_min, self.is_explore] = load(joblib_file)\n",
    "    \n",
    "Goal_vs_Hole_v0.load_world = load_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display episode count\n",
    "def print_episode(episode, delay=1):\n",
    "    os.system('clear')\n",
    "    for _ in range(13):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    print(\"Episode \", episode)\n",
    "    for _ in range(13):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display the world, delay of 1 sec for ease of understanding\n",
    "def print_status(the_world, done, step, delay=1):\n",
    "    os.system('clear')\n",
    "    the_world.print_world(action, step)\n",
    "    the_world.print_q_table()\n",
    "    if done:\n",
    "        print(\"-------EPISODE DONE--------\")\n",
    "        delay *= 2\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "do_training = True\n",
    "do_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if do_training==True:\n",
    "    maxwins = 2000\n",
    "    delay = 0\n",
    "    wins = 0\n",
    "    episode_count = 10 * maxwins\n",
    "    # scores (max number of steps bef goal) - good indicator of learning\n",
    "    scores = deque(maxlen=maxwins)\n",
    "    goal_vs_hole_v0_world = Goal_vs_Hole_v0()\n",
    "    \n",
    "    \n",
    "    step = 1\n",
    "    exit_flag = False\n",
    "    # state, action, reward, next state iteration\n",
    "    for episode in range(episode_count):\n",
    "        state = goal_vs_hole_v0_world.reset()\n",
    "        done = False\n",
    "        print_episode(episode, delay=delay)\n",
    "        while not done:\n",
    "            action = goal_vs_hole_v0_world.act()\n",
    "            next_state, reward, done = goal_vs_hole_v0_world.step(action)\n",
    "            goal_vs_hole_v0_world.update_q_table(state, action, reward, next_state)\n",
    "            print_status(goal_vs_hole_v0_world, done, step, delay=delay)\n",
    "            state = next_state\n",
    "            # if episode is done, perform housekeeping\n",
    "            if done:\n",
    "                if goal_vs_hole_v0_world.is_in_win_state():\n",
    "                    wins += 1\n",
    "                    scores.append(step)\n",
    "                    if wins > maxwins:\n",
    "                        exit_flag = True\n",
    "                # Exploration-Exploitation is updated every episode\n",
    "                goal_vs_hole_v0_world.update_epsilon()\n",
    "                step = 1\n",
    "            else:\n",
    "                step += 1\n",
    "            if exit_flag==True:\n",
    "                break\n",
    "        if exit_flag == True:\n",
    "            break\n",
    "    #print(\"Done..\")\n",
    "    print(\"Saving world for future use...\")\n",
    "    goal_vs_hole_v0_world.save_world()\n",
    "    print(scores)\n",
    "    goal_vs_hole_v0_world.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"video/non-gym-2x3-grid-training.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"video/non-gym-2x3-grid-training.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if do_test == True:\n",
    "    maxwins = 10\n",
    "    delay = 1\n",
    "    \n",
    "    wins = 0\n",
    "    episode_count = 10 * maxwins\n",
    "    # scores (max number of steps bef goal) - good indicator of learning\n",
    "    scores = deque(maxlen=maxwins)\n",
    "    \n",
    "    goal_vs_hole_v0_world = Goal_vs_Hole_v0(start_state=0)\n",
    "    \n",
    "    \n",
    "    #Load pre-existing Q-table\n",
    "    print(\"Loading a world...\")\n",
    "    goal_vs_hole_v0_world.load_world()\n",
    "\n",
    "    step = 1\n",
    "    exit_flag = False\n",
    "    # state, action, reward, next state iteration\n",
    "    for episode in range(episode_count):\n",
    "        start_state = np.random.choice(goal_vs_hole_v0_world.observation_space_non_terminal)\n",
    "        state = goal_vs_hole_v0_world.reset(start_state = start_state)\n",
    "        done = False\n",
    "        print_episode(episode, delay=delay)\n",
    "        while not done:\n",
    "            action = goal_vs_hole_v0_world.act()\n",
    "            next_state, reward, done = goal_vs_hole_v0_world.step(action)\n",
    "            #goal_vs_hole_v0_world.update_q_table(state, action, reward, next_state)\n",
    "            print_status(goal_vs_hole_v0_world, done, step, delay=delay)\n",
    "            state = next_state\n",
    "            # if episode is done, perform housekeeping\n",
    "            if done:\n",
    "                if goal_vs_hole_v0_world.is_in_win_state():\n",
    "                    wins += 1\n",
    "                    scores.append(step)\n",
    "                    if wins > maxwins:\n",
    "                        exit_flag = True\n",
    "                # Exploration-Exploitation is updated every episode\n",
    "                goal_vs_hole_v0_world.update_epsilon(do_test=do_test)\n",
    "                step = 1\n",
    "            else:\n",
    "                step += 1\n",
    "            if exit_flag==True:\n",
    "                break\n",
    "        if exit_flag == True:\n",
    "            break\n",
    "    #print(\"Done..\")\n",
    "    print(scores)\n",
    "    goal_vs_hole_v0_world.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"video/non-gym-2x3-grid-testing.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"video/non-gym-2x3-grid-testing.mov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks for your attention"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": 855,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "width": 1280
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
