{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![what is RL](figs/what_is_rl.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Reinforcement Learning with OpenAI Gym\n",
    "## About Gym\n",
    "* `Gym` is an open source Python library for developing and comparing `reinforcement learning` algorithms\n",
    "* It provides a standard API to communicate between the learning algorithms and environments.\n",
    "* It also provides a standard set of environments compliant with that API.\n",
    "* `Gym` documentation website is located [https://www.gymlibrary.dev/](https://www.gymlibrary.dev/). \n",
    "* `Gym` also has a discord server for development purposes that you can join [https://discord.gg/nHg2JRN489](https://discord.gg/nHg2JRN489).\n",
    "* `Gym`'s official developer site is [https://github.com/openai/gym](https://github.com/openai/gym).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gymlibrary.dev](figs/gymlibrary.dev.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gym environment categories\n",
    "1. Atari\n",
    "2. MuJoCo\n",
    "3. Toy Text\n",
    "4. Classic Control\n",
    "5. Box2D\n",
    "6. Third party environments\n",
    "7. Create your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Atari Gym environments\n",
    "* A set of Atari 2600 environment simulated through Stella and the Arcade Learning Environment.\n",
    "* Atari environments are simulated via the Arcade Learning Environment (ALE).\n",
    "* List of Atari 2600 games: [https://en.wikipedia.org/wiki/List_of_Atari_2600_games](https://en.wikipedia.org/wiki/List_of_Atari_2600_games)\n",
    "* Just in case you are looking for ROMs to revisit past [http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html](http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html)\n",
    "\n",
    "![Atari-2600](figs/Atari.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. MuJoCo Gym environments\n",
    "* MuJoCo stands for **Mu**lti-**Jo**int dynamics with **Co**ntact. \n",
    "* It is a physics engine for faciliatating research and development in robotics, biomechanics, graphics and animation, and other areas where fast and accurate simulation is needed.\n",
    "* The unique dependencies for this set of environments can be installed via: `pip install gym[mujoco]`\n",
    "\n",
    "![Mujoco](figs/mujoco.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Toy Text Gym environments\n",
    "* All toy text environments were created by us using native Python libraries such as StringIO.\n",
    "* These environments are designed to be extremely simple, with small discrete state and action spaces, and hence easy to learn. \n",
    "* As a result, they are suitable for debugging implementations of reinforcement learning algorithms.\n",
    "\n",
    "![toytext](figs/toytext.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4. Classic Control Gym environments\n",
    "* The unique dependencies for this set of environments can be installed via: `pip install gym[classic_control]` .\n",
    "* There are five classic control environments: `Acrobot`, `CartPole`, `Mountain Car`, `Continuous Mountain Car`, and `Pendulum`. \n",
    "* All of these environments are stochastic in terms of their initial state, within a given range. \n",
    "* In addition, `Acrobot` has noise applied to the taken action. \n",
    "* Also, regarding the both `mountain car` environments, the cars are under powered to climb the mountain, so it takes some effort to reach the top.\n",
    "* Among Gym environments, this set of environments can be considered as easier ones to solve by a `policy`.\n",
    "\n",
    "![classic-control](figs/classic-control2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5. Box2D Gym environments\n",
    "* These environments all involve toy games based around physics control, using box2d based physics and PyGame based rendering. \n",
    "* These environments were contributed back in the early days of Gym by Oleg Klimov, and have become popular toy benchmarks ever since. \n",
    "* All environments are highly configurable via arguments specified in each environment’s documentation.\n",
    "* The unique dependencies for this set of environments can be installed via: `pip install gym[box2d]`\n",
    "\n",
    "![box2d](figs/box2d.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6. Third party Gym environments\n",
    "* OpenAI Gym also hosts a good number of third party environments. For example,\n",
    "* `ViZDoom` :  An environment centered around the original Doom game.\n",
    "    - It focuses on visual control (from image to actions) at thousands of frames per second.\n",
    "* `MineRL` : Gym interface with Minecraft game, focused on a specific sparse reward challenge.\n",
    "* `GymGo` : The board game Go, also known as Weiqi. The game that was famously conquered by AlphaGo.\n",
    "* `gym-gazebo` : gym-gazebo presents an extension of the initial OpenAI gym for robotics using ROS and Gazebo, an advanced 3D modeling and rendering tool.\n",
    "* `PettingZoo` : PettingZoo is a Python library for conducting research in multi-agent reinforcement learning, akin to a multi-agent version of Gym.\n",
    "* `gym-recsys` : This package describes an OpenAI Gym interface for creating a simulation environment of reinforcement learning-based recommender systems (RL-RecSys). The design strives for simple and flexible APIs to support novel research.\n",
    "* `math-prog-synth-env` : In our paper “A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis” we convert the DeepMind Mathematics Dataset into an RL environment based around program.\n",
    "* `NLPGym`: A toolkit to develop RL agents to solve NLP tasks. NLPGym provides interactive environments for standard NLP tasks such as sequence tagging, question answering, and sequence classification. Users can easily customize the tasks with their own datasets, observations, featurizers and reward functions.\n",
    "* ...\n",
    "* and many more\n",
    "* Detailed list can be obtained [https://www.gymlibrary.dev/environments/third_party_environments/](https://www.gymlibrary.dev/environments/third_party_environments/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 7. Create your own Gym environments\n",
    "* If you are interested to construct a gym environment yourself and behave the same way as all other gym environments, OpenAI gym allows you to do that too with ease\n",
    "    - Here is a short tutorial that you can follow [https://www.gymlibrary.dev/content/environment_creation/](https://www.gymlibrary.dev/content/environment_creation/)\n",
    "\n",
    "![custom-gym-env](figs/custom-gym-env.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installation\n",
    "* `pip install gym`\n",
    "\n",
    "This does not include dependencies for all families of environments (there's a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like `pip install gym[atari]` or use the following to install all dependencies.: \n",
    "* `pip install gym[all]`\n",
    "\n",
    "If the above pip install throws a bash/zsh error, it might be the subscripts not allowed there. You need to set option for that.\n",
    "* `setopt no_nomatch`\n",
    "\n",
    "Then run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gym[pong]\n"
     ]
    }
   ],
   "source": [
    "#!setopt no_nomatch\n",
    "#!pip install gym[pong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load, dump\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Listing available `gym` environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/ashis/venv-directory/venv-ml-p3.10/bin/python3.10\n",
    "#Please make this python file executable and then run it without passing \n",
    "# it to python interpreter as the the interpreter listed on the first line \n",
    "# will be invoked. Good luck!\n",
    "#$ chmod +x list_all_envs_registry.py\n",
    "#$ ./list_all_envs_registry.py\n",
    "\n",
    "from gym import envs\n",
    "#all_envs = envs.registry.all()\n",
    "#env_ids = [env_spec.id for env_spec in all_envs]\n",
    "#pprint(sorted(env_ids))\n",
    "for key in envs.registry.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 environment keys:\n",
      "       ALE/Adventure-v5\n",
      "0  ALE/Adventure-ram-v5\n",
      "1        ALE/AirRaid-v5\n",
      "2    ALE/AirRaid-ram-v5\n",
      "3          ALE/Alien-v5\n",
      "4      ALE/Alien-ram-v5\n",
      "5         ALE/Amidar-v5\n",
      "6     ALE/Amidar-ram-v5\n",
      "7        ALE/Assault-v5\n",
      "8    ALE/Assault-ram-v5\n",
      "9        ALE/Asterix-v5\n",
      "...\n",
      "Last 10 environment keys:\n",
      "       ALE/Adventure-v5\n",
      "985         Walker2d-v3\n",
      "986         Walker2d-v4\n",
      "987              Ant-v2\n",
      "988              Ant-v3\n",
      "989              Ant-v4\n",
      "990         Humanoid-v2\n",
      "991         Humanoid-v3\n",
      "992         Humanoid-v4\n",
      "993  HumanoidStandup-v2\n",
      "994  HumanoidStandup-v4\n"
     ]
    }
   ],
   "source": [
    "list_keys = pd.read_csv('../list_all_envs_registry.txt')\n",
    "print('First 10 environment keys:')\n",
    "print(list_keys.head(n=10))\n",
    "print('...')\n",
    "print('Last 10 environment keys:')\n",
    "print(list_keys.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interacting with the Environment\n",
    "Gym implements the classic “agent-environment loop”:\n",
    "\n",
    "<img src=\"https://www.gymlibrary.dev/_images/AE_loop_dark.png\" style=\"background-color:black;\" width=300> \n",
    "\n",
    "Image courtesy: www.gymlibrary.dev\n",
    "\n",
    "The agent performs some `actions` in the environment (usually by passing some control inputs to the environment, e.g. torque inputs of motors) and observes how the `environment’s state` changes. One such action-observation exchange is referred to as a `timestep`.\n",
    "\n",
    "The goal in Reinforcement Learning (RL) is to manipulate the `environment` in some specific way. \n",
    "\n",
    "For instance, we want the agent to navigate a robot to a specific point in space. \n",
    "* If it succeeds in doing this (or makes some progress towards that goal), it will receive a `positive reward` alongside the observation for this `timestep`. \n",
    "* The reward may also be negative or 0, if the agent did not yet succeed (or did not make any progress). \n",
    "* The agent will then be trained to maximize the reward it accumulates over many timesteps.\n",
    "* After some timesteps, the environment may enter a terminal state. \n",
    "    * For instance, the robot may have crashed! In that case, we want to `reset the environment` to a new initial state. The environment issues a done signal to the agent if it enters such a terminal state. \n",
    "    * Not all done signals must be triggered by a “catastrophic failure”: Sometimes we also want to issue a done signal after a fixed number of timesteps, or if the agent has succeeded in completing some task in the environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agent-Environment loop in `Gym`\n",
    "* Here below is an example of agent-environment loop in `gym`: `LunarLander-v2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LunarLander-v2\n",
    "* Reference [https://www.gymlibrary.dev/environments/box2d/lunar_lander/](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "![lunarlander-v2](figs/lunar_landerv2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LunarLander-v2\n",
    "\n",
    "* This example will run an instance of `LunarLander-v2` environment for `n` timesteps. \n",
    "* Since we pass `render_mode=\"human\"`, you should see a window pop up rendering the environment.\n",
    "* Save the following in a file named `lunarlanderv2.py`\n",
    "\n",
    "![lunarlander-v2](../video/lunarlander-v2-random.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/ashis/venv-directory/venv-ml-p3.10/bin/python3.10\n",
    "#Please make this python file executable and then run it without passing it to python interpreter\n",
    "#as the the interpreter listed on the first line will be invoked. Good luck!\n",
    "#$ chmod +x lunarlanderv2.py\n",
    "#$ ./lunralanderv2.py\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "#number of timesteps\n",
    "n = 500\n",
    "\n",
    "#Since we pass render_mode=\"human\", you should see a window pop up rendering the environment.\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in tqdm(range(n)):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        #break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Action space and Observation (i.e., state) space\n",
    "* Every environment specifies the format of valid actions by providing an `env.action_space` attribute. \n",
    "* Similarly, the format of valid observations is specified by `env.observation_space`. \n",
    "* In the example above we sampled random actions via `env.action_space.sample()`. \n",
    "* Note that we need to seed the action space separately from the environment to ensure reproducible samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks for your attention"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": 855,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "width": 1280
  },
  "vscode": {
   "interpreter": {
    "hash": "d93dfb006d9629cf586318b929ece614012e52132ab19faf5910bcf241a5c1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
