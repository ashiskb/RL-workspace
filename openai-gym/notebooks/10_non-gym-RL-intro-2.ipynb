{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-gym environment\n",
    "## Action-Environment Loop (From scratch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Non-Gym game development (Part 2)\n",
    "## Goal vs Hole - v1\n",
    "* * Simple 2D text-based game with the following environment:\n",
    "\n",
    "```text\n",
    "--------------------------------------------\n",
    "|         |          |          |          |\n",
    "|  Start  |          |          |          |\n",
    "|         |          |          |          |\n",
    "--------------------------------------------\n",
    "|         |          |          |          |\n",
    "|         |          |  Hole    |          |\n",
    "|         |          |          |          |\n",
    "--------------------------------------------\n",
    "|         |          |          |          |\n",
    "|         |  Hole    |  Goal    |          |\n",
    "|         |          |          |          |\n",
    "--------------------------------------------\n",
    "|         |          |          |          |\n",
    "|         |          |          |  Hole    |\n",
    "|         |          |          |          |\n",
    "--------------------------------------------\n",
    "```\n",
    "* The environment is a 4x4 grid.\n",
    "* 4 terminal states: one `Goal` and three `Hole` states; if a player moves into any of the four terminal states, the game is over.\n",
    "* 12 non-terminal states.\n",
    "* **Rewards**\n",
    "    * 0 if on a non-terminal state,\n",
    "    * -100 if on `Hole` state,\n",
    "    * +100 if on `Goal` state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#the imports\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from termcolor import colored\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Let's define a class\n",
    "class Goal_vs_Hole_v1():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def init_reward_table(self):\n",
    "    \"\"\"\n",
    "        0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    --------------------------------------------\n",
    "    |         |          |          |          |\n",
    "    |  Start  |          |          |          |\n",
    "    |    0    |    1     |    2     |    3     |\n",
    "    --------------------------------------------\n",
    "    |         |          |  -100    |          |\n",
    "    |         |          |  Hole    |          |\n",
    "    |    4    |    5     |    6     |    7     |\n",
    "    --------------------------------------------\n",
    "    |         |  -100    |  +100    |          |\n",
    "    |         |  Hole    |  Goal    |          |\n",
    "    |    8    |    9     |    10    |    11    |\n",
    "    --------------------------------------------\n",
    "    |         |          |          |  -100    |\n",
    "    |         |          |          |  Hole    |\n",
    "    |   12    |   13     |    14    |    15    |\n",
    "    --------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    self.reward_table = np.zeros([self.row, self.col])\n",
    "    self.reward_table[5, 2] = -100.\n",
    "    self.reward_table[5, 1] = -100.\n",
    "    self.reward_table[2, 1] = -100.\n",
    "    self.reward_table[7, 0] = -100.\n",
    "    self.reward_table[8, 2] = -100.\n",
    "    self.reward_table[11,0] = 100.\n",
    "    self.reward_table[11,1] = -100.\n",
    "    self.reward_table[13,3] = -100.\n",
    "    self.reward_table[14,3] = 100.\n",
    "    self.reward_table[14,2] = -100.\n",
    "\n",
    "Goal_vs_Hole_v1.init_reward_table = init_reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def init_transition_table(self):\n",
    "    \"\"\" TT[state_{i},action] = state_{i+1}\n",
    "        0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "        ------------------\n",
    "        | 0 | 1 | 2   | 3 |\n",
    "        -------------------\n",
    "        | 4 | 5 | 6H  | 7 |\n",
    "        -------------------\n",
    "        | 8 | 9H | 10G| 11| \n",
    "        -------------------\n",
    "        |12 | 13 | 14 |15H|\n",
    "        -------------------\n",
    "    \"\"\"\n",
    "    left, down, right, up = (0,1,2,3)\n",
    "    self.action_space = [left,down,right,up]\n",
    "    \n",
    "    self.observation_space = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    self.observation_space_terminal = [6,9,10,15]\n",
    "    self.observation_space_non_terminal = [0,1,2,3,4,5,7,8,11,12,13,14]\n",
    "    \n",
    "    \n",
    "    self.transition_table = np.zeros([self.row, self.col], dtype=int)\n",
    "\n",
    "    T = np.zeros_like(self.transition_table)\n",
    "    #non-goal and non-hole transitions\n",
    "    T[0,left], T[0,right], T[0,up], T[0, down] = (0,1,0,4)\n",
    "    T[1,left], T[1,right], T[1,up], T[1, down] = (0,2,1,5)\n",
    "    T[2,left], T[2,right], T[2,up], T[2, down] = (1,3,2,6)\n",
    "    T[3,left], T[3,right], T[3,up], T[3, down] = (2,3,3,7)\n",
    "    T[4,left], T[4,right], T[4,up], T[4, down] = (4,5,0,8)\n",
    "    T[5,left], T[5,right], T[5,up], T[5, down] = (4,6,1,9)\n",
    "    T[7,left], T[7,right], T[7,up], T[7, down] = (6,7,3,11)\n",
    "    T[8,left], T[8,right], T[8,up], T[8, down] = (8,9,4,12)\n",
    "    T[11,left], T[11,right], T[11,up], T[11, down] = (10,11,7,15)\n",
    "    T[12,left], T[12,right], T[12,up], T[12, down] = (12,13,8,12)\n",
    "    T[13,left], T[13,right], T[13,up], T[13, down] = (12,14,9,13)\n",
    "    T[14,left], T[14,right], T[14,up], T[14, down] = (13,15,10,14)\n",
    "\n",
    "    # terminal Goal state\n",
    "    T[10,left], T[10,right], T[10,up], T[10,down] = (10,10,10,10)\n",
    "\n",
    "    # terminal Hole state\n",
    "    T[6,left], T[6,right], T[6,up], T[6, down] = (6,6,6,6)\n",
    "    T[9,left], T[9,right], T[9,up], T[9, down] = (9,9,9,9)\n",
    "    T[15,left], T[15,right], T[15,up], T[15, down] = (15,15,15,15)\n",
    "\n",
    "    self.transition_table = T\n",
    "    \n",
    "Goal_vs_Hole_v1.init_transition_table = init_transition_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# start of episode\n",
    "def reset(self, start_state=0):\n",
    "    self.state = start_state\n",
    "    return self.state\n",
    "\n",
    "Goal_vs_Hole_v1.reset = reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(self, start_state=0):\n",
    "    # 4 actions\n",
    "    # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    self.col = 4\n",
    "    #self.action_space = (4,)\n",
    "\n",
    "    # 16 states, 4x4 grid\n",
    "    self.row = 16\n",
    "    #self.observation_space = (4,4)\n",
    "\n",
    "    # setup the environment\n",
    "    self.q_table = np.zeros([self.row, self.col])\n",
    "    self.init_transition_table()\n",
    "    self.init_reward_table()\n",
    "\n",
    "    # discount factor\n",
    "    self.gamma = 0.9\n",
    "\n",
    "    # 90% exploration, 10% exploitation\n",
    "    self.epsilon = 0.9\n",
    "    # exploration decays by this factor every episode\n",
    "    self.epsilon_decay = 0.99\n",
    "    # in the long run, 10% exploration, 90% exploitation\n",
    "    # meaning, the agent is never going to bore you with predictable moves :D\n",
    "    self.epsilon_min = 0.1\n",
    "\n",
    "    # reset the environment\n",
    "    self.reset()\n",
    "    self.is_explore = True\n",
    "    \n",
    "    \n",
    "Goal_vs_Hole_v1.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# agent wins when the goal is reached\n",
    "def is_in_win_state(self):\n",
    "    return self.state == 10\n",
    "Goal_vs_Hole_v1.is_in_win_state = is_in_win_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# execute the action on the environment\n",
    "def step(self, action, verbose=False):\n",
    "    if verbose:\n",
    "        print('self.transition_table')\n",
    "        print(self.transition_table)\n",
    "    # determine the next_state given state and action\n",
    "    next_state = self.transition_table[self.state, action]\n",
    "\n",
    "    # done is True if next_state is Goal or Hole\n",
    "    done = next_state == 6 or next_state == 9 or next_state == 10 or next_state == 15\n",
    "\n",
    "    # reward given the state and action\n",
    "    reward = self.reward_table[self.state, action]\n",
    "\n",
    "    # the enviroment is now in new state\n",
    "    self.state = next_state\n",
    "\n",
    "    return next_state, reward, done\n",
    "\n",
    "Goal_vs_Hole_v1.step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# determine the next action\n",
    "def act(self):\n",
    "    # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "\n",
    "    # action is from exploration, by following the distribution \"epsilon\"\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "        # explore - do random action\n",
    "        self.is_explore = True\n",
    "\n",
    "        #find valid transitions from current state\n",
    "        valid_actions_from_state = np.where(self.transition_table[self.state,:]!=self.state)\n",
    "        return np.random.choice(valid_actions_from_state[0])\n",
    "\n",
    "    # otherwise,  action is from exploitation\n",
    "    # exploit - choose action with max Q-value\n",
    "    self.is_explore = False\n",
    "\n",
    "    return np.argmax(self.q_table[self.state])\n",
    "\n",
    "Goal_vs_Hole_v1.act = act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Q-Learning - update the Q Table using Q(s, a)\n",
    "def update_q_table(self, state, action, reward, next_state):\n",
    "    # Remember the Bellman equation to update the Q table?\n",
    "    # Q(s, a) = reward + gamma * max_a' Q(s', a')\n",
    "    q_value = reward + self.gamma * np.amax(self.q_table[next_state])\n",
    "\n",
    "    self.q_table[state, action] = q_value\n",
    "    \n",
    "Goal_vs_Hole_v1.update_q_table = update_q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# update Exploration-Exploitation mix\n",
    "def update_epsilon(self, do_test=False):\n",
    "    if do_test:\n",
    "        self.epsilon = 0.0 #absolutely no exploration during test time\n",
    "    else:\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "Goal_vs_Hole_v1.update_epsilon = update_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def state_coord(self, state):\n",
    "    return (state//4, 2+4*(state%4))\n",
    "    if state==0: return (0,2)\n",
    "    elif state==1: return (0,6)\n",
    "    elif state==2: return (0,10)\n",
    "    elif state==3: return (0,14)\n",
    "    \n",
    "Goal_vs_Hole_v1.state_coord = state_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display agent moving on the grid\n",
    "# Playing with the terminal, so you could see \"kinda\" animation!\n",
    "def print_cell(self, row=0):\n",
    "    print(\"\")\n",
    "    for i in range(17): #col\n",
    "        if self.state in [6,9,10,15]: #current state is one of the terminal states\n",
    "            hole_cells = [(6,10,1),(9,6,2),(15,14,3)]\n",
    "            goal_cells = [(10,10,2)]\n",
    "            flag = False\n",
    "            color_goal = 'green'\n",
    "            if (self.state,i,row) in hole_cells:\n",
    "                marker = \"\\033[4mH\\033[0m\"\n",
    "                flag = True\n",
    "            elif (self.state,i,row) in goal_cells:\n",
    "                marker = \"\\033[4mG\\033[0m\"\n",
    "                flag = True\n",
    "            elif (i,row) in [(10,1),(6,2),(14,3)]:\n",
    "                marker = 'H' #hole state\n",
    "                flag= True\n",
    "            elif (i,row) in [(10,2)]:\n",
    "                marker = 'G' #goal state\n",
    "                flag = True\n",
    "\n",
    "            if i in [1,3,5,7,9,11,13,15] and flag==False: #none of the above gets printed\n",
    "                marker = ' ' \n",
    "            elif i in [2,6,10,14] and flag==False:\n",
    "                marker = ' '\n",
    "            elif flag==False:\n",
    "                marker = ''\n",
    "                \n",
    "            if (self.state,i,row) in goal_cells:\n",
    "                color_goal = 'red'\n",
    "            elif (self.state,i,row) in hole_cells:\n",
    "                color_goal = 'blue'\n",
    "            else:\n",
    "                color_goal = 'green'\n",
    "            print(colored(marker, color_goal), end='')\n",
    "        else: #current state is a non-terminal state\n",
    "            color_goal = 'green'\n",
    "            H_G_flag = False\n",
    "            r,c = self.state_coord(self.state)\n",
    "            if (r,c) in [(row,i)]:\n",
    "                marker = '_'\n",
    "                color_goal = 'red'\n",
    "                H_G_flag = True\n",
    "            elif (i,row) in [(10,1),(6,2),(14,3)]:\n",
    "                marker = 'H' #hole state\n",
    "                H_G_flag = True\n",
    "            elif (i,row) in [(10,2)]:\n",
    "                marker = 'G' #goal state\n",
    "                H_G_flag = True\n",
    "\n",
    "            if i in [1,3,5,7,9,11,13,15] and H_G_flag==False: #none of the above gets printed\n",
    "                marker = ' ' \n",
    "            elif i in [2,6,10,14] and H_G_flag==False:\n",
    "                marker = ' '\n",
    "            elif H_G_flag==False:\n",
    "                marker = ''\n",
    "            print(colored(marker, color_goal), end='')\n",
    "\n",
    "        if i % 4 == 0: #every 4 col, close the cell with a boundary\n",
    "            print('|', end='')\n",
    "    print(\"\")    \n",
    "    \n",
    "Goal_vs_Hole_v1.print_cell = print_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display mode and action of agent\n",
    "def print_world(self, action, step):\n",
    "    actions = {0: \"(Left)\", 1: \"(Down)\", 2: \"(Right)\", 3: \"(Up)\"}\n",
    "    explore = \"Explore\" if self.is_explore else \"Exploit\"\n",
    "    print(\"Step\", step, \":\", explore, actions[action])\n",
    "    for _ in range(17):\n",
    "        print('-', end='')\n",
    "    self.print_cell() #row=0\n",
    "    for _ in range(17):\n",
    "        print('-', end='')\n",
    "    for r in [1,2,3]: #remaining rows\n",
    "        self.print_cell(row=r) #print row `r`\n",
    "        for _ in range(17):\n",
    "            print('-', end='')\n",
    "    print(\"\")\n",
    "\n",
    "Goal_vs_Hole_v1.print_world = print_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print Q Table contents\n",
    "def print_q_table(self):\n",
    "    print(\"Q-Table (Epsilon: %0.2f)\" % self.epsilon)\n",
    "    print(self.q_table)\n",
    "Goal_vs_Hole_v1.print_q_table = print_q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# save member variables in a pickle\n",
    "def save_world(self, joblib_file='joblibs/goal_vs_hole_world_v1.joblib'):\n",
    "    dump([self.col, self.row, self.q_table, self.gamma, self.epsilon, \n",
    "        self.epsilon_decay, self.epsilon_min, self.is_explore], joblib_file)\n",
    "\n",
    "Goal_vs_Hole_v1.save_world = save_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load member variables from a pickle file\n",
    "def load_world(self, joblib_file = 'joblibs/goal_vs_hole_world_v1.joblib'):\n",
    "    [self.col, self.row, self.q_table, self.gamma, self.epsilon,\n",
    "            self.epsilon_decay, self.epsilon_min, self.is_explore] = load(joblib_file)\n",
    "    \n",
    "Goal_vs_Hole_v1.load_world = load_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display episode count\n",
    "def print_episode(episode, delay=1):\n",
    "    os.system('clear')\n",
    "    for _ in range(17):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    print(\"Episode \", episode)\n",
    "    for _ in range(17):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# UI to display the world, delay of 1 sec for ease of understanding\n",
    "def print_status(the_world, done, step, delay=1,training_mode=True):\n",
    "    os.system('clear')\n",
    "    the_world.print_world(action, step)\n",
    "    if training_mode: the_world.print_q_table()\n",
    "    if done:\n",
    "        print(\"-------EPISODE DONE--------\")\n",
    "        delay *= 2\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "do_training = True\n",
    "do_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if do_training==True:\n",
    "    maxwins = 100\n",
    "    delay = 0\n",
    "    wins = 0\n",
    "    episode_count = 10 * maxwins\n",
    "    # scores (max number of steps bef goal) - good indicator of learning\n",
    "    scores = deque(maxlen=maxwins)\n",
    "    goal_vs_hole_v1_world = Goal_vs_Hole_v1()\n",
    "    \n",
    "    \n",
    "    step = 1\n",
    "    exit_flag = False\n",
    "    # state, action, reward, next state iteration\n",
    "    for episode in range(episode_count):\n",
    "        state = goal_vs_hole_v1_world.reset()\n",
    "        done = False\n",
    "        print_episode(episode, delay=delay)\n",
    "        while not done:\n",
    "            action = goal_vs_hole_v1_world.act()\n",
    "            next_state, reward, done = goal_vs_hole_v1_world.step(action)\n",
    "            goal_vs_hole_v1_world.update_q_table(state, action, reward, next_state)\n",
    "            print_status(goal_vs_hole_v1_world, done, step, delay=delay)\n",
    "            state = next_state\n",
    "            # if episode is done, perform housekeeping\n",
    "            if done:\n",
    "                if goal_vs_hole_v1_world.is_in_win_state():\n",
    "                    wins += 1\n",
    "                    scores.append(step)\n",
    "                    if wins > maxwins:\n",
    "                        exit_flag = True\n",
    "                # Exploration-Exploitation is updated every episode\n",
    "                goal_vs_hole_v1_world.update_epsilon()\n",
    "                step = 1\n",
    "            else:\n",
    "                step += 1\n",
    "            if exit_flag==True:\n",
    "                break\n",
    "        if exit_flag == True:\n",
    "            break\n",
    "    #print(\"Done..\")\n",
    "    print(\"Saving world for future use...\")\n",
    "    goal_vs_hole_v1_world.save_world()\n",
    "    print(scores)\n",
    "    goal_vs_hole_v1_world.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"video/non-gym-4x4-grid-training.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"video/non-gym-4x4-grid-training.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if do_test == True:\n",
    "    maxwins = 10\n",
    "    delay = 1\n",
    "    \n",
    "    wins = 0\n",
    "    episode_count = 10 * maxwins\n",
    "    # scores (max number of steps bef goal) - good indicator of learning\n",
    "    scores = deque(maxlen=maxwins)\n",
    "    \n",
    "    goal_vs_hole_v1_world = Goal_vs_Hole_v1(start_state=0)\n",
    "    \n",
    "    \n",
    "    #Load pre-existing Q-table\n",
    "    print(\"Loading a world...\")\n",
    "    goal_vs_hole_v1_world.load_world()\n",
    "\n",
    "    step = 1\n",
    "    exit_flag = False\n",
    "    # state, action, reward, next state iteration\n",
    "    for episode in range(episode_count):\n",
    "        start_state = np.random.choice(goal_vs_hole_v1_world.observation_space_non_terminal)\n",
    "        state = goal_vs_hole_v1_world.reset(start_state = start_state)\n",
    "        done = False\n",
    "        print_episode(episode, delay=delay)\n",
    "        while not done:\n",
    "            action = goal_vs_hole_v1_world.act()\n",
    "            next_state, reward, done = goal_vs_hole_v1_world.step(action)\n",
    "            #goal_vs_hole_v1_world.update_q_table(state, action, reward, next_state)\n",
    "            print_status(goal_vs_hole_v1_world, done, step, delay=delay)\n",
    "            state = next_state\n",
    "            # if episode is done, perform housekeeping\n",
    "            if done:\n",
    "                if goal_vs_hole_v1_world.is_in_win_state():\n",
    "                    wins += 1\n",
    "                    scores.append(step)\n",
    "                    if wins > maxwins:\n",
    "                        exit_flag = True\n",
    "                # Exploration-Exploitation is updated every episode\n",
    "                goal_vs_hole_v1_world.update_epsilon(do_test=do_test)\n",
    "                step = 1\n",
    "            else:\n",
    "                step += 1\n",
    "            if exit_flag==True:\n",
    "                break\n",
    "        if exit_flag == True:\n",
    "            break\n",
    "    #print(\"Done..\")\n",
    "    print(scores)\n",
    "    goal_vs_hole_v1_world.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"video/non-gym-4x4-grid-testing.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"video/non-gym-4x4-grid-testing.mov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks for your attention"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": 855,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "width": 1280
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
